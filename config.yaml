models:
  - model: Qwen/Qwen2.5-Coder-7B-Instruct
    parameters:
      density: 0.6
      weight: 1.0
  - model: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
    parameters:
      density: 0.6
      weight: 0.9
  - model: google/gemma-2-27b-it
    parameters:
      density: 0.5
      weight: 0.8
  - model: codellama/CodeLlama-34b-Instruct
    parameters:
      density: 0.5
      weight: 0.7
  - model: microsoft/Phi-3-medium-4k-instruct
    parameters:
      density: 0.4
      weight: 0.6
  - model: 01-ai/Yi-34B-Chat
    parameters:
      density: 0.4
      weight: 0.5
  - model: mistralai/Mixtral-8x7B-Instruct-v0.1
    parameters:
      density: 0.3
      weight: 0.5
  - model: NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO
    parameters:
      density: 0.3
      weight: 0.4

merge_method: linear
base_model: Qwen/Qwen2.5-Coder-7B-Instruct
parameters:
  normalize: true
  int8_mask: true
dtype: bfloat16
out_path: BharatDev-v1  # Tera final model folder yahan banega
